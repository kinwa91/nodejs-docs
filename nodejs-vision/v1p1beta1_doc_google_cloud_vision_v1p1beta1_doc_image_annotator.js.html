<!doctype html>
<html>

<head>
  <meta name="generator" content="JSDoc 3.5.5">
  <meta charset="utf-8">
  <title>@google-cloud/vision 0.23.0 &raquo; Source: v1p1beta1/doc/google/cloud/vision/v1p1beta1/doc_image_annotator.js</title>
  <link rel="stylesheet" href="https://brick.a.ssl.fastly.net/Karla:400,400i,700,700i" type="text/css">
  <link rel="stylesheet" href="https://brick.a.ssl.fastly.net/Noto+Serif:400,400i,700,700i" type="text/css">
  <link rel="stylesheet" href="https://brick.a.ssl.fastly.net/Inconsolata:500" type="text/css">
  <link href="css/baseline.css" rel="stylesheet">
</head>

<body onload="prettyPrint()">
  <div id="toc-nav-container">
    <div id="toc-nav">
      hi
    </div>
  </div>
  <div id="jsdoc-body-container">
    <div id="jsdoc-content">
      <div id="jsdoc-content-container">
        <div id="jsdoc-banner" role="banner">
        </div>
        <div id="jsdoc-main" role="main">
          <header class="page-header">
            <h1>Source: v1p1beta1/doc/google/cloud/vision/v1p1beta1/doc_image_annotator.js</h1>
          </header>
          <article>
            <pre class="prettyprint linenums"><code>// Copyright 2018 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     https://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// Note: this file is purely for documentation. Any contents are not expected
// to be loaded as the JS file.

/**
 * Users describe the type of Google Cloud Vision API tasks to perform over
 * images by using *Feature*s. Each Feature indicates a type of image
 * detection task to perform. Features encode the Cloud Vision API
 * vertical to operate on and the number of top-scoring results to return.
 *
 * @property {number} type
 *   The feature type.
 *
 *   The number should be among the values of [Type]{@link google.cloud.vision.v1p1beta1.Type}
 *
 * @property {number} maxResults
 *   Maximum number of results of this type.
 *
 * @property {string} model
 *   Model to use for the feature.
 *   Supported values: &quot;builtin/stable&quot; (the default if unset) and
 *   &quot;builtin/latest&quot;.
 *
 * @typedef Feature
 * @memberof google.cloud.vision.v1p1beta1
 * @see [google.cloud.vision.v1p1beta1.Feature definition in proto format]{@link https://github.com/googleapis/googleapis/blob/master/google/cloud/vision/v1p1beta1/image_annotator.proto}
 */
const Feature &#x3D; {
  // This is for documentation. Actual contents will be loaded by gRPC.

  /**
   * Type of image feature.
   *
   * @enum {number}
   * @memberof google.cloud.vision.v1p1beta1
   */
  Type: {

    /**
     * Unspecified feature type.
     */
    TYPE_UNSPECIFIED: 0,

    /**
     * Run face detection.
     */
    FACE_DETECTION: 1,

    /**
     * Run landmark detection.
     */
    LANDMARK_DETECTION: 2,

    /**
     * Run logo detection.
     */
    LOGO_DETECTION: 3,

    /**
     * Run label detection.
     */
    LABEL_DETECTION: 4,

    /**
     * Run OCR.
     */
    TEXT_DETECTION: 5,

    /**
     * Run dense text document OCR. Takes precedence when both
     * DOCUMENT_TEXT_DETECTION and TEXT_DETECTION are present.
     */
    DOCUMENT_TEXT_DETECTION: 11,

    /**
     * Run computer vision models to compute image safe-search properties.
     */
    SAFE_SEARCH_DETECTION: 6,

    /**
     * Compute a set of image properties, such as the image&#x27;s dominant colors.
     */
    IMAGE_PROPERTIES: 7,

    /**
     * Run crop hints.
     */
    CROP_HINTS: 9,

    /**
     * Run web detection.
     */
    WEB_DETECTION: 10
  }
};

/**
 * External image source (Google Cloud Storage image location).
 *
 * @property {string} gcsImageUri
 *   NOTE: For new code &#x60;image_uri&#x60; below is preferred.
 *   Google Cloud Storage image URI, which must be in the following form:
 *   &#x60;gs://bucket_name/object_name&#x60; (for details, see
 *   [Google Cloud Storage Request
 *   URIs](https://cloud.google.com/storage/docs/reference-uris)).
 *   NOTE: Cloud Storage object versioning is not supported.
 *
 * @property {string} imageUri
 *   Image URI which supports:
 *   1) Google Cloud Storage image URI, which must be in the following form:
 *   &#x60;gs://bucket_name/object_name&#x60; (for details, see
 *   [Google Cloud Storage Request
 *   URIs](https://cloud.google.com/storage/docs/reference-uris)).
 *   NOTE: Cloud Storage object versioning is not supported.
 *   2) Publicly accessible image HTTP/HTTPS URL.
 *   This is preferred over the legacy &#x60;gcs_image_uri&#x60; above. When both
 *   &#x60;gcs_image_uri&#x60; and &#x60;image_uri&#x60; are specified, &#x60;image_uri&#x60; takes
 *   precedence.
 *
 * @typedef ImageSource
 * @memberof google.cloud.vision.v1p1beta1
 * @see [google.cloud.vision.v1p1beta1.ImageSource definition in proto format]{@link https://github.com/googleapis/googleapis/blob/master/google/cloud/vision/v1p1beta1/image_annotator.proto}
 */
const ImageSource &#x3D; {
  // This is for documentation. Actual contents will be loaded by gRPC.
};

/**
 * Client image to perform Google Cloud Vision API tasks over.
 *
 * @property {string} content
 *   Image content, represented as a stream of bytes.
 *   Note: as with all &#x60;bytes&#x60; fields, protobuffers use a pure binary
 *   representation, whereas JSON representations use base64.
 *
 * @property {Object} source
 *   Google Cloud Storage image location. If both &#x60;content&#x60; and &#x60;source&#x60;
 *   are provided for an image, &#x60;content&#x60; takes precedence and is
 *   used to perform the image annotation request.
 *
 *   This object should have the same structure as [ImageSource]{@link google.cloud.vision.v1p1beta1.ImageSource}
 *
 * @typedef Image
 * @memberof google.cloud.vision.v1p1beta1
 * @see [google.cloud.vision.v1p1beta1.Image definition in proto format]{@link https://github.com/googleapis/googleapis/blob/master/google/cloud/vision/v1p1beta1/image_annotator.proto}
 */
const Image &#x3D; {
  // This is for documentation. Actual contents will be loaded by gRPC.
};

/**
 * A face annotation object contains the results of face detection.
 *
 * @property {Object} boundingPoly
 *   The bounding polygon around the face. The coordinates of the bounding box
 *   are in the original image&#x27;s scale, as returned in &#x60;ImageParams&#x60;.
 *   The bounding box is computed to &quot;frame&quot; the face in accordance with human
 *   expectations. It is based on the landmarker results.
 *   Note that one or more x and/or y coordinates may not be generated in the
 *   &#x60;BoundingPoly&#x60; (the polygon will be unbounded) if only a partial face
 *   appears in the image to be annotated.
 *
 *   This object should have the same structure as [BoundingPoly]{@link google.cloud.vision.v1p1beta1.BoundingPoly}
 *
 * @property {Object} fdBoundingPoly
 *   The &#x60;fd_bounding_poly&#x60; bounding polygon is tighter than the
 *   &#x60;boundingPoly&#x60;, and encloses only the skin part of the face. Typically, it
 *   is used to eliminate the face from any image analysis that detects the
 *   &quot;amount of skin&quot; visible in an image. It is not based on the
 *   landmarker results, only on the initial face detection, hence
 *   the &amp;lt;code&gt;fd&amp;lt;/code&gt; (face detection) prefix.
 *
 *   This object should have the same structure as [BoundingPoly]{@link google.cloud.vision.v1p1beta1.BoundingPoly}
 *
 * @property {Object[]} landmarks
 *   Detected face landmarks.
 *
 *   This object should have the same structure as [Landmark]{@link google.cloud.vision.v1p1beta1.Landmark}
 *
 * @property {number} rollAngle
 *   Roll angle, which indicates the amount of clockwise/anti-clockwise rotation
 *   of the face relative to the image vertical about the axis perpendicular to
 *   the face. Range [-180,180].
 *
 * @property {number} panAngle
 *   Yaw angle, which indicates the leftward/rightward angle that the face is
 *   pointing relative to the vertical plane perpendicular to the image. Range
 *   [-180,180].
 *
 * @property {number} tiltAngle
 *   Pitch angle, which indicates the upwards/downwards angle that the face is
 *   pointing relative to the image&#x27;s horizontal plane. Range [-180,180].
 *
 * @property {number} detectionConfidence
 *   Detection confidence. Range [0, 1].
 *
 * @property {number} landmarkingConfidence
 *   Face landmarking confidence. Range [0, 1].
 *
 * @property {number} joyLikelihood
 *   Joy likelihood.
 *
 *   The number should be among the values of [Likelihood]{@link google.cloud.vision.v1p1beta1.Likelihood}
 *
 * @property {number} sorrowLikelihood
 *   Sorrow likelihood.
 *
 *   The number should be among the values of [Likelihood]{@link google.cloud.vision.v1p1beta1.Likelihood}
 *
 * @property {number} angerLikelihood
 *   Anger likelihood.
 *
 *   The number should be among the values of [Likelihood]{@link google.cloud.vision.v1p1beta1.Likelihood}
 *
 * @property {number} surpriseLikelihood
 *   Surprise likelihood.
 *
 *   The number should be among the values of [Likelihood]{@link google.cloud.vision.v1p1beta1.Likelihood}
 *
 * @property {number} underExposedLikelihood
 *   Under-exposed likelihood.
 *
 *   The number should be among the values of [Likelihood]{@link google.cloud.vision.v1p1beta1.Likelihood}
 *
 * @property {number} blurredLikelihood
 *   Blurred likelihood.
 *
 *   The number should be among the values of [Likelihood]{@link google.cloud.vision.v1p1beta1.Likelihood}
 *
 * @property {number} headwearLikelihood
 *   Headwear likelihood.
 *
 *   The number should be among the values of [Likelihood]{@link google.cloud.vision.v1p1beta1.Likelihood}
 *
 * @typedef FaceAnnotation
 * @memberof google.cloud.vision.v1p1beta1
 * @see [google.cloud.vision.v1p1beta1.FaceAnnotation definition in proto format]{@link https://github.com/googleapis/googleapis/blob/master/google/cloud/vision/v1p1beta1/image_annotator.proto}
 */
const FaceAnnotation &#x3D; {
  // This is for documentation. Actual contents will be loaded by gRPC.

  /**
   * A face-specific landmark (for example, a face feature).
   *
   * @property {number} type
   *   Face landmark type.
   *
   *   The number should be among the values of [Type]{@link google.cloud.vision.v1p1beta1.Type}
   *
   * @property {Object} position
   *   Face landmark position.
   *
   *   This object should have the same structure as [Position]{@link google.cloud.vision.v1p1beta1.Position}
   *
   * @typedef Landmark
   * @memberof google.cloud.vision.v1p1beta1
   * @see [google.cloud.vision.v1p1beta1.FaceAnnotation.Landmark definition in proto format]{@link https://github.com/googleapis/googleapis/blob/master/google/cloud/vision/v1p1beta1/image_annotator.proto}
   */
  Landmark: {
    // This is for documentation. Actual contents will be loaded by gRPC.

    /**
     * Face landmark (feature) type.
     * Left and right are defined from the vantage of the viewer of the image
     * without considering mirror projections typical of photos. So, &#x60;LEFT_EYE&#x60;,
     * typically, is the person&#x27;s right eye.
     *
     * @enum {number}
     * @memberof google.cloud.vision.v1p1beta1
     */
    Type: {

      /**
       * Unknown face landmark detected. Should not be filled.
       */
      UNKNOWN_LANDMARK: 0,

      /**
       * Left eye.
       */
      LEFT_EYE: 1,

      /**
       * Right eye.
       */
      RIGHT_EYE: 2,

      /**
       * Left of left eyebrow.
       */
      LEFT_OF_LEFT_EYEBROW: 3,

      /**
       * Right of left eyebrow.
       */
      RIGHT_OF_LEFT_EYEBROW: 4,

      /**
       * Left of right eyebrow.
       */
      LEFT_OF_RIGHT_EYEBROW: 5,

      /**
       * Right of right eyebrow.
       */
      RIGHT_OF_RIGHT_EYEBROW: 6,

      /**
       * Midpoint between eyes.
       */
      MIDPOINT_BETWEEN_EYES: 7,

      /**
       * Nose tip.
       */
      NOSE_TIP: 8,

      /**
       * Upper lip.
       */
      UPPER_LIP: 9,

      /**
       * Lower lip.
       */
      LOWER_LIP: 10,

      /**
       * Mouth left.
       */
      MOUTH_LEFT: 11,

      /**
       * Mouth right.
       */
      MOUTH_RIGHT: 12,

      /**
       * Mouth center.
       */
      MOUTH_CENTER: 13,

      /**
       * Nose, bottom right.
       */
      NOSE_BOTTOM_RIGHT: 14,

      /**
       * Nose, bottom left.
       */
      NOSE_BOTTOM_LEFT: 15,

      /**
       * Nose, bottom center.
       */
      NOSE_BOTTOM_CENTER: 16,

      /**
       * Left eye, top boundary.
       */
      LEFT_EYE_TOP_BOUNDARY: 17,

      /**
       * Left eye, right corner.
       */
      LEFT_EYE_RIGHT_CORNER: 18,

      /**
       * Left eye, bottom boundary.
       */
      LEFT_EYE_BOTTOM_BOUNDARY: 19,

      /**
       * Left eye, left corner.
       */
      LEFT_EYE_LEFT_CORNER: 20,

      /**
       * Right eye, top boundary.
       */
      RIGHT_EYE_TOP_BOUNDARY: 21,

      /**
       * Right eye, right corner.
       */
      RIGHT_EYE_RIGHT_CORNER: 22,

      /**
       * Right eye, bottom boundary.
       */
      RIGHT_EYE_BOTTOM_BOUNDARY: 23,

      /**
       * Right eye, left corner.
       */
      RIGHT_EYE_LEFT_CORNER: 24,

      /**
       * Left eyebrow, upper midpoint.
       */
      LEFT_EYEBROW_UPPER_MIDPOINT: 25,

      /**
       * Right eyebrow, upper midpoint.
       */
      RIGHT_EYEBROW_UPPER_MIDPOINT: 26,

      /**
       * Left ear tragion.
       */
      LEFT_EAR_TRAGION: 27,

      /**
       * Right ear tragion.
       */
      RIGHT_EAR_TRAGION: 28,

      /**
       * Left eye pupil.
       */
      LEFT_EYE_PUPIL: 29,

      /**
       * Right eye pupil.
       */
      RIGHT_EYE_PUPIL: 30,

      /**
       * Forehead glabella.
       */
      FOREHEAD_GLABELLA: 31,

      /**
       * Chin gnathion.
       */
      CHIN_GNATHION: 32,

      /**
       * Chin left gonion.
       */
      CHIN_LEFT_GONION: 33,

      /**
       * Chin right gonion.
       */
      CHIN_RIGHT_GONION: 34
    }
  }
};

/**
 * Detected entity location information.
 *
 * @property {Object} latLng
 *   lat/long location coordinates.
 *
 *   This object should have the same structure as [LatLng]{@link google.type.LatLng}
 *
 * @typedef LocationInfo
 * @memberof google.cloud.vision.v1p1beta1
 * @see [google.cloud.vision.v1p1beta1.LocationInfo definition in proto format]{@link https://github.com/googleapis/googleapis/blob/master/google/cloud/vision/v1p1beta1/image_annotator.proto}
 */
const LocationInfo &#x3D; {
  // This is for documentation. Actual contents will be loaded by gRPC.
};

/**
 * A &#x60;Property&#x60; consists of a user-supplied name/value pair.
 *
 * @property {string} name
 *   Name of the property.
 *
 * @property {string} value
 *   Value of the property.
 *
 * @property {number} uint64Value
 *   Value of numeric properties.
 *
 * @typedef Property
 * @memberof google.cloud.vision.v1p1beta1
 * @see [google.cloud.vision.v1p1beta1.Property definition in proto format]{@link https://github.com/googleapis/googleapis/blob/master/google/cloud/vision/v1p1beta1/image_annotator.proto}
 */
const Property &#x3D; {
  // This is for documentation. Actual contents will be loaded by gRPC.
};

/**
 * Set of detected entity features.
 *
 * @property {string} mid
 *   Opaque entity ID. Some IDs may be available in
 *   [Google Knowledge Graph Search API](https://developers.google.com/knowledge-graph/).
 *
 * @property {string} locale
 *   The language code for the locale in which the entity textual
 *   &#x60;description&#x60; is expressed.
 *
 * @property {string} description
 *   Entity textual description, expressed in its &#x60;locale&#x60; language.
 *
 * @property {number} score
 *   Overall score of the result. Range [0, 1].
 *
 * @property {number} confidence
 *   The accuracy of the entity detection in an image.
 *   For example, for an image in which the &quot;Eiffel Tower&quot; entity is detected,
 *   this field represents the confidence that there is a tower in the query
 *   image. Range [0, 1].
 *
 * @property {number} topicality
 *   The relevancy of the ICA (Image Content Annotation) label to the
 *   image. For example, the relevancy of &quot;tower&quot; is likely higher to an image
 *   containing the detected &quot;Eiffel Tower&quot; than to an image containing a
 *   detected distant towering building, even though the confidence that
 *   there is a tower in each image may be the same. Range [0, 1].
 *
 * @property {Object} boundingPoly
 *   Image region to which this entity belongs. Not produced
 *   for &#x60;LABEL_DETECTION&#x60; features.
 *
 *   This object should have the same structure as [BoundingPoly]{@link google.cloud.vision.v1p1beta1.BoundingPoly}
 *
 * @property {Object[]} locations
 *   The location information for the detected entity. Multiple
 *   &#x60;LocationInfo&#x60; elements can be present because one location may
 *   indicate the location of the scene in the image, and another location
 *   may indicate the location of the place where the image was taken.
 *   Location information is usually present for landmarks.
 *
 *   This object should have the same structure as [LocationInfo]{@link google.cloud.vision.v1p1beta1.LocationInfo}
 *
 * @property {Object[]} properties
 *   Some entities may have optional user-supplied &#x60;Property&#x60; (name/value)
 *   fields, such a score or string that qualifies the entity.
 *
 *   This object should have the same structure as [Property]{@link google.cloud.vision.v1p1beta1.Property}
 *
 * @typedef EntityAnnotation
 * @memberof google.cloud.vision.v1p1beta1
 * @see [google.cloud.vision.v1p1beta1.EntityAnnotation definition in proto format]{@link https://github.com/googleapis/googleapis/blob/master/google/cloud/vision/v1p1beta1/image_annotator.proto}
 */
const EntityAnnotation &#x3D; {
  // This is for documentation. Actual contents will be loaded by gRPC.
};

/**
 * Set of features pertaining to the image, computed by computer vision
 * methods over safe-search verticals (for example, adult, spoof, medical,
 * violence).
 *
 * @property {number} adult
 *   Represents the adult content likelihood for the image. Adult content may
 *   contain elements such as nudity, pornographic images or cartoons, or
 *   sexual activities.
 *
 *   The number should be among the values of [Likelihood]{@link google.cloud.vision.v1p1beta1.Likelihood}
 *
 * @property {number} spoof
 *   Spoof likelihood. The likelihood that an modification
 *   was made to the image&#x27;s canonical version to make it appear
 *   funny or offensive.
 *
 *   The number should be among the values of [Likelihood]{@link google.cloud.vision.v1p1beta1.Likelihood}
 *
 * @property {number} medical
 *   Likelihood that this is a medical image.
 *
 *   The number should be among the values of [Likelihood]{@link google.cloud.vision.v1p1beta1.Likelihood}
 *
 * @property {number} violence
 *   Likelihood that this image contains violent content.
 *
 *   The number should be among the values of [Likelihood]{@link google.cloud.vision.v1p1beta1.Likelihood}
 *
 * @property {number} racy
 *   Likelihood that the request image contains racy content. Racy content may
 *   include (but is not limited to) skimpy or sheer clothing, strategically
 *   covered nudity, lewd or provocative poses, or close-ups of sensitive
 *   body areas.
 *
 *   The number should be among the values of [Likelihood]{@link google.cloud.vision.v1p1beta1.Likelihood}
 *
 * @typedef SafeSearchAnnotation
 * @memberof google.cloud.vision.v1p1beta1
 * @see [google.cloud.vision.v1p1beta1.SafeSearchAnnotation definition in proto format]{@link https://github.com/googleapis/googleapis/blob/master/google/cloud/vision/v1p1beta1/image_annotator.proto}
 */
const SafeSearchAnnotation &#x3D; {
  // This is for documentation. Actual contents will be loaded by gRPC.
};

/**
 * Rectangle determined by min and max &#x60;LatLng&#x60; pairs.
 *
 * @property {Object} minLatLng
 *   Min lat/long pair.
 *
 *   This object should have the same structure as [LatLng]{@link google.type.LatLng}
 *
 * @property {Object} maxLatLng
 *   Max lat/long pair.
 *
 *   This object should have the same structure as [LatLng]{@link google.type.LatLng}
 *
 * @typedef LatLongRect
 * @memberof google.cloud.vision.v1p1beta1
 * @see [google.cloud.vision.v1p1beta1.LatLongRect definition in proto format]{@link https://github.com/googleapis/googleapis/blob/master/google/cloud/vision/v1p1beta1/image_annotator.proto}
 */
const LatLongRect &#x3D; {
  // This is for documentation. Actual contents will be loaded by gRPC.
};

/**
 * Color information consists of RGB channels, score, and the fraction of
 * the image that the color occupies in the image.
 *
 * @property {Object} color
 *   RGB components of the color.
 *
 *   This object should have the same structure as [Color]{@link google.type.Color}
 *
 * @property {number} score
 *   Image-specific score for this color. Value in range [0, 1].
 *
 * @property {number} pixelFraction
 *   The fraction of pixels the color occupies in the image.
 *   Value in range [0, 1].
 *
 * @typedef ColorInfo
 * @memberof google.cloud.vision.v1p1beta1
 * @see [google.cloud.vision.v1p1beta1.ColorInfo definition in proto format]{@link https://github.com/googleapis/googleapis/blob/master/google/cloud/vision/v1p1beta1/image_annotator.proto}
 */
const ColorInfo &#x3D; {
  // This is for documentation. Actual contents will be loaded by gRPC.
};

/**
 * Set of dominant colors and their corresponding scores.
 *
 * @property {Object[]} colors
 *   RGB color values with their score and pixel fraction.
 *
 *   This object should have the same structure as [ColorInfo]{@link google.cloud.vision.v1p1beta1.ColorInfo}
 *
 * @typedef DominantColorsAnnotation
 * @memberof google.cloud.vision.v1p1beta1
 * @see [google.cloud.vision.v1p1beta1.DominantColorsAnnotation definition in proto format]{@link https://github.com/googleapis/googleapis/blob/master/google/cloud/vision/v1p1beta1/image_annotator.proto}
 */
const DominantColorsAnnotation &#x3D; {
  // This is for documentation. Actual contents will be loaded by gRPC.
};

/**
 * Stores image properties, such as dominant colors.
 *
 * @property {Object} dominantColors
 *   If present, dominant colors completed successfully.
 *
 *   This object should have the same structure as [DominantColorsAnnotation]{@link google.cloud.vision.v1p1beta1.DominantColorsAnnotation}
 *
 * @typedef ImageProperties
 * @memberof google.cloud.vision.v1p1beta1
 * @see [google.cloud.vision.v1p1beta1.ImageProperties definition in proto format]{@link https://github.com/googleapis/googleapis/blob/master/google/cloud/vision/v1p1beta1/image_annotator.proto}
 */
const ImageProperties &#x3D; {
  // This is for documentation. Actual contents will be loaded by gRPC.
};

/**
 * Single crop hint that is used to generate a new crop when serving an image.
 *
 * @property {Object} boundingPoly
 *   The bounding polygon for the crop region. The coordinates of the bounding
 *   box are in the original image&#x27;s scale, as returned in &#x60;ImageParams&#x60;.
 *
 *   This object should have the same structure as [BoundingPoly]{@link google.cloud.vision.v1p1beta1.BoundingPoly}
 *
 * @property {number} confidence
 *   Confidence of this being a salient region.  Range [0, 1].
 *
 * @property {number} importanceFraction
 *   Fraction of importance of this salient region with respect to the original
 *   image.
 *
 * @typedef CropHint
 * @memberof google.cloud.vision.v1p1beta1
 * @see [google.cloud.vision.v1p1beta1.CropHint definition in proto format]{@link https://github.com/googleapis/googleapis/blob/master/google/cloud/vision/v1p1beta1/image_annotator.proto}
 */
const CropHint &#x3D; {
  // This is for documentation. Actual contents will be loaded by gRPC.
};

/**
 * Set of crop hints that are used to generate new crops when serving images.
 *
 * @property {Object[]} cropHints
 *   Crop hint results.
 *
 *   This object should have the same structure as [CropHint]{@link google.cloud.vision.v1p1beta1.CropHint}
 *
 * @typedef CropHintsAnnotation
 * @memberof google.cloud.vision.v1p1beta1
 * @see [google.cloud.vision.v1p1beta1.CropHintsAnnotation definition in proto format]{@link https://github.com/googleapis/googleapis/blob/master/google/cloud/vision/v1p1beta1/image_annotator.proto}
 */
const CropHintsAnnotation &#x3D; {
  // This is for documentation. Actual contents will be loaded by gRPC.
};

/**
 * Parameters for crop hints annotation request.
 *
 * @property {number[]} aspectRatios
 *   Aspect ratios in floats, representing the ratio of the width to the height
 *   of the image. For example, if the desired aspect ratio is 4/3, the
 *   corresponding float value should be 1.33333.  If not specified, the
 *   best possible crop is returned. The number of provided aspect ratios is
 *   limited to a maximum of 16; any aspect ratios provided after the 16th are
 *   ignored.
 *
 * @typedef CropHintsParams
 * @memberof google.cloud.vision.v1p1beta1
 * @see [google.cloud.vision.v1p1beta1.CropHintsParams definition in proto format]{@link https://github.com/googleapis/googleapis/blob/master/google/cloud/vision/v1p1beta1/image_annotator.proto}
 */
const CropHintsParams &#x3D; {
  // This is for documentation. Actual contents will be loaded by gRPC.
};

/**
 * Parameters for web detection request.
 *
 * @property {boolean} includeGeoResults
 *   Whether to include results derived from the geo information in the image.
 *
 * @typedef WebDetectionParams
 * @memberof google.cloud.vision.v1p1beta1
 * @see [google.cloud.vision.v1p1beta1.WebDetectionParams definition in proto format]{@link https://github.com/googleapis/googleapis/blob/master/google/cloud/vision/v1p1beta1/image_annotator.proto}
 */
const WebDetectionParams &#x3D; {
  // This is for documentation. Actual contents will be loaded by gRPC.
};

/**
 * Image context and/or feature-specific parameters.
 *
 * @property {Object} latLongRect
 *   lat/long rectangle that specifies the location of the image.
 *
 *   This object should have the same structure as [LatLongRect]{@link google.cloud.vision.v1p1beta1.LatLongRect}
 *
 * @property {string[]} languageHints
 *   List of languages to use for TEXT_DETECTION. In most cases, an empty value
 *   yields the best results since it enables automatic language detection. For
 *   languages based on the Latin alphabet, setting &#x60;language_hints&#x60; is not
 *   needed. In rare cases, when the language of the text in the image is known,
 *   setting a hint will help get better results (although it will be a
 *   significant hindrance if the hint is wrong). Text detection returns an
 *   error if one or more of the specified languages is not one of the
 *   [supported languages](https://cloud.google.com/vision/docs/languages).
 *
 * @property {Object} cropHintsParams
 *   Parameters for crop hints annotation request.
 *
 *   This object should have the same structure as [CropHintsParams]{@link google.cloud.vision.v1p1beta1.CropHintsParams}
 *
 * @property {Object} webDetectionParams
 *   Parameters for web detection.
 *
 *   This object should have the same structure as [WebDetectionParams]{@link google.cloud.vision.v1p1beta1.WebDetectionParams}
 *
 * @typedef ImageContext
 * @memberof google.cloud.vision.v1p1beta1
 * @see [google.cloud.vision.v1p1beta1.ImageContext definition in proto format]{@link https://github.com/googleapis/googleapis/blob/master/google/cloud/vision/v1p1beta1/image_annotator.proto}
 */
const ImageContext &#x3D; {
  // This is for documentation. Actual contents will be loaded by gRPC.
};

/**
 * Request for performing Google Cloud Vision API tasks over a user-provided
 * image, with user-requested features.
 *
 * @property {Object} image
 *   The image to be processed.
 *
 *   This object should have the same structure as [Image]{@link google.cloud.vision.v1p1beta1.Image}
 *
 * @property {Object[]} features
 *   Requested features.
 *
 *   This object should have the same structure as [Feature]{@link google.cloud.vision.v1p1beta1.Feature}
 *
 * @property {Object} imageContext
 *   Additional context that may accompany the image.
 *
 *   This object should have the same structure as [ImageContext]{@link google.cloud.vision.v1p1beta1.ImageContext}
 *
 * @typedef AnnotateImageRequest
 * @memberof google.cloud.vision.v1p1beta1
 * @see [google.cloud.vision.v1p1beta1.AnnotateImageRequest definition in proto format]{@link https://github.com/googleapis/googleapis/blob/master/google/cloud/vision/v1p1beta1/image_annotator.proto}
 */
const AnnotateImageRequest &#x3D; {
  // This is for documentation. Actual contents will be loaded by gRPC.
};

/**
 * Response to an image annotation request.
 *
 * @property {Object[]} faceAnnotations
 *   If present, face detection has completed successfully.
 *
 *   This object should have the same structure as [FaceAnnotation]{@link google.cloud.vision.v1p1beta1.FaceAnnotation}
 *
 * @property {Object[]} landmarkAnnotations
 *   If present, landmark detection has completed successfully.
 *
 *   This object should have the same structure as [EntityAnnotation]{@link google.cloud.vision.v1p1beta1.EntityAnnotation}
 *
 * @property {Object[]} logoAnnotations
 *   If present, logo detection has completed successfully.
 *
 *   This object should have the same structure as [EntityAnnotation]{@link google.cloud.vision.v1p1beta1.EntityAnnotation}
 *
 * @property {Object[]} labelAnnotations
 *   If present, label detection has completed successfully.
 *
 *   This object should have the same structure as [EntityAnnotation]{@link google.cloud.vision.v1p1beta1.EntityAnnotation}
 *
 * @property {Object[]} textAnnotations
 *   If present, text (OCR) detection has completed successfully.
 *
 *   This object should have the same structure as [EntityAnnotation]{@link google.cloud.vision.v1p1beta1.EntityAnnotation}
 *
 * @property {Object} fullTextAnnotation
 *   If present, text (OCR) detection or document (OCR) text detection has
 *   completed successfully.
 *   This annotation provides the structural hierarchy for the OCR detected
 *   text.
 *
 *   This object should have the same structure as [TextAnnotation]{@link google.cloud.vision.v1p1beta1.TextAnnotation}
 *
 * @property {Object} safeSearchAnnotation
 *   If present, safe-search annotation has completed successfully.
 *
 *   This object should have the same structure as [SafeSearchAnnotation]{@link google.cloud.vision.v1p1beta1.SafeSearchAnnotation}
 *
 * @property {Object} imagePropertiesAnnotation
 *   If present, image properties were extracted successfully.
 *
 *   This object should have the same structure as [ImageProperties]{@link google.cloud.vision.v1p1beta1.ImageProperties}
 *
 * @property {Object} cropHintsAnnotation
 *   If present, crop hints have completed successfully.
 *
 *   This object should have the same structure as [CropHintsAnnotation]{@link google.cloud.vision.v1p1beta1.CropHintsAnnotation}
 *
 * @property {Object} webDetection
 *   If present, web detection has completed successfully.
 *
 *   This object should have the same structure as [WebDetection]{@link google.cloud.vision.v1p1beta1.WebDetection}
 *
 * @property {Object} error
 *   If set, represents the error message for the operation.
 *   Note that filled-in image annotations are guaranteed to be
 *   correct, even when &#x60;error&#x60; is set.
 *
 *   This object should have the same structure as [Status]{@link google.rpc.Status}
 *
 * @typedef AnnotateImageResponse
 * @memberof google.cloud.vision.v1p1beta1
 * @see [google.cloud.vision.v1p1beta1.AnnotateImageResponse definition in proto format]{@link https://github.com/googleapis/googleapis/blob/master/google/cloud/vision/v1p1beta1/image_annotator.proto}
 */
const AnnotateImageResponse &#x3D; {
  // This is for documentation. Actual contents will be loaded by gRPC.
};

/**
 * Multiple image annotation requests are batched into a single service call.
 *
 * @property {Object[]} requests
 *   Individual image annotation requests for this batch.
 *
 *   This object should have the same structure as [AnnotateImageRequest]{@link google.cloud.vision.v1p1beta1.AnnotateImageRequest}
 *
 * @typedef BatchAnnotateImagesRequest
 * @memberof google.cloud.vision.v1p1beta1
 * @see [google.cloud.vision.v1p1beta1.BatchAnnotateImagesRequest definition in proto format]{@link https://github.com/googleapis/googleapis/blob/master/google/cloud/vision/v1p1beta1/image_annotator.proto}
 */
const BatchAnnotateImagesRequest &#x3D; {
  // This is for documentation. Actual contents will be loaded by gRPC.
};

/**
 * Response to a batch image annotation request.
 *
 * @property {Object[]} responses
 *   Individual responses to image annotation requests within the batch.
 *
 *   This object should have the same structure as [AnnotateImageResponse]{@link google.cloud.vision.v1p1beta1.AnnotateImageResponse}
 *
 * @typedef BatchAnnotateImagesResponse
 * @memberof google.cloud.vision.v1p1beta1
 * @see [google.cloud.vision.v1p1beta1.BatchAnnotateImagesResponse definition in proto format]{@link https://github.com/googleapis/googleapis/blob/master/google/cloud/vision/v1p1beta1/image_annotator.proto}
 */
const BatchAnnotateImagesResponse &#x3D; {
  // This is for documentation. Actual contents will be loaded by gRPC.
};

/**
 * A bucketized representation of likelihood, which is intended to give clients
 * highly stable results across model upgrades.
 *
 * @enum {number}
 * @memberof google.cloud.vision.v1p1beta1
 */
const Likelihood &#x3D; {

  /**
   * Unknown likelihood.
   */
  UNKNOWN: 0,

  /**
   * It is very unlikely that the image belongs to the specified vertical.
   */
  VERY_UNLIKELY: 1,

  /**
   * It is unlikely that the image belongs to the specified vertical.
   */
  UNLIKELY: 2,

  /**
   * It is possible that the image belongs to the specified vertical.
   */
  POSSIBLE: 3,

  /**
   * It is likely that the image belongs to the specified vertical.
   */
  LIKELY: 4,

  /**
   * It is very likely that the image belongs to the specified vertical.
   */
  VERY_LIKELY: 5
};</code></pre>
          </article>
        </div>
      </div>
      <nav id="jsdoc-toc-nav" role="navigation"></nav>
    </div>
  </div>
  <footer id="jsdoc-footer" class="jsdoc-footer">
    <div id="jsdoc-footer-container">
      <p>
        Generated by <a href="https://github.com/jsdoc3/jsdoc">JSDoc</a> 3.5.5 on November 16, 2018.
      </p>
    </div>
  </footer>
  <script src="scripts/jquery.min.js"></script>
  <script src="scripts/jquery.cookie.js"></script>
  <script src="scripts/tree.jquery.js"></script>
  <script src="scripts/prettify.js"></script>
  <script src="scripts/jsdoc-toc.js"></script>
  <script src="scripts/linenumber.js"></script>
  <script src="scripts/scrollanchor.js"></script>
</body>

</html>